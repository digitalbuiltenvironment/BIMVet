{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m nb_classifier \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mnb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# End timing\u001b[39;00m\n\u001b[0;32m     60\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\naive_bayes.py:759\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    757\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32mc:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\naive_bayes.py:881\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\utils\\validation.py:1660\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1660\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"../datasets/train_set2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define a constant for missing values\n",
    "EMPTYCONST = \"*empty*\"\n",
    "\n",
    "# Concatenate relevant text features\n",
    "catFamily = df[\"Family\"].fillna(EMPTYCONST)\n",
    "catSubFamily = df[\"SubFamily\"].fillna(EMPTYCONST)\n",
    "catObjectGroup = df[\"ObjectGroup\"].fillna(EMPTYCONST)\n",
    "catObjectName = df[\"ObjectName\"].fillna(EMPTYCONST)\n",
    "catDescription = df[\"Description\"].fillna(EMPTYCONST)\n",
    "catTypeComments = df[\"Type Comments\"].fillna(EMPTYCONST)\n",
    "catStructuralMaterial = df[\"Structural Material\"].fillna(EMPTYCONST)\n",
    "catMaterial = df[\"Material\"].fillna(EMPTYCONST)\n",
    "\n",
    "# Create a new feature combining all text features\n",
    "df[\"Features\"] = (\n",
    "    catFamily + \"|\" +\n",
    "    catSubFamily + \"|\" +\n",
    "    catObjectGroup + \"|\" +\n",
    "    catObjectName + \"|\" +\n",
    "    catDescription + \"|\" +\n",
    "    catTypeComments + \"|\" +\n",
    "    catStructuralMaterial + \"|\" +\n",
    "    catMaterial\n",
    ")\n",
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = df[\"Features\"]\n",
    "y = df[\"Category\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Vectorize text data using HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "# Scale data using MaxAbsScaler (handles sparse input)\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_tfidf = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_tfidf = scaler.transform(X_test_tfidf)\n",
    "# Check vectorized data\n",
    "\n",
    "\n",
    "# Initialize Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total runtime\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total runtime: {total_time} seconds\")\n",
    "# Make predictions on the test set\n",
    "predictions = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_filename = \"nb_hv.pkl\"\n",
    "joblib.dump(nb_classifier, model_filename)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
