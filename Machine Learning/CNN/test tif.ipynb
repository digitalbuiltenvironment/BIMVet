{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Flatten, Dropout, Conv1D, MaxPooling1D\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv1d_20. Consider increasing the input size. Received input shape [None, 1, 3034] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train_reshaped)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Define the neural network model\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m model_nn \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMaxPooling1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMaxPooling1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     82\u001b[0m model_nn\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\convolutional\\base_conv.py:347\u001b[0m, in \u001b[0;36mConv.compute_output_shape\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape(\n\u001b[0;32m    341\u001b[0m             input_shape[:batch_rank]\n\u001b[0;32m    342\u001b[0m             \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters]\n\u001b[0;32m    343\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spatial_output_shape(input_shape[batch_rank \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :])\n\u001b[0;32m    344\u001b[0m         )\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    348\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of the dimensions in the output is <= 0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to downsampling in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincreasing the input size. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived input shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which would produce \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput shape with a zero or negative value in a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv1d_20. Consider increasing the input size. Received input shape [None, 1, 3034] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame(data)\n",
    "file_path = \"../datasets/train_set2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "EMPTYCONST = \"*empty*\"\n",
    "\n",
    "# Concatenate relevant text features\n",
    "catFamily = df[\"Family\"].fillna(EMPTYCONST)\n",
    "catSubFamily = df[\"SubFamily\"].fillna(EMPTYCONST)\n",
    "catObjectGroup = df[\"ObjectGroup\"].fillna(EMPTYCONST)\n",
    "catObjectName = df[\"ObjectName\"].fillna(EMPTYCONST)\n",
    "catDescription = df[\"Description\"].fillna(EMPTYCONST)\n",
    "catTypeComments = df[\"Type Comments\"].fillna(EMPTYCONST)\n",
    "catStructuralMaterial = df[\"Structural Material\"].fillna(EMPTYCONST)\n",
    "catMaterial = df[\"Material\"].fillna(EMPTYCONST)\n",
    "\n",
    "\n",
    "df[\"Features\"] = (\n",
    "    catFamily\n",
    "    + \"|\"\n",
    "    + catSubFamily\n",
    "    + \"|\"\n",
    "    + catObjectGroup\n",
    "    + \"|\"\n",
    "    + catObjectName\n",
    "    + \"|\"\n",
    "    + catDescription\n",
    "    + \"|\"\n",
    "    + catTypeComments\n",
    "    + \"|\"\n",
    "    + catStructuralMaterial\n",
    "    + \"|\"\n",
    "    + catMaterial\n",
    ")\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Encode categorical variable 'Category'\n",
    "label_encoder = LabelEncoder()\n",
    "df['Category'] = label_encoder.fit_transform(df['Category'])\n",
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = df[\"Features\"].fillna(EMPTYCONST)\n",
    "y = df[\"Category\"]\n",
    "\n",
    "# Vectorize text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Convert X_tfidf to a dense numpy array\n",
    "X_tfidf_dense = X_tfidf.toarray()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf_dense, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "timesteps = 1  # Each sample is treated as a single time step\n",
    "features = X_train.shape[1]  # The number of features is the dimensionality of the TF-IDF vector\n",
    "\n",
    "X_train_reshaped = np.expand_dims(X_train, axis=1)  # Shape (batch_size, timesteps, features)\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=1)    # Shape (batch_size, timesteps, features)\n",
    "\n",
    "# Define the neural network model\n",
    "model_nn = Sequential([\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', input_shape=(timesteps, features)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(df['Category'].unique()), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model_nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_nn.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total runtime\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total runtime: {total_time} seconds\")\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "loss, accuracy = model_nn.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Save the model to disk\n",
    "model_nn.save('CNN_model_tif.h5')\n",
    "print(\"Model saved to disk.\")\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 38ms/step\n",
      "Accuracy: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       1.0\n",
      "           1       0.00      0.00      0.00       1.0\n",
      "           4       0.00      0.00      0.00      22.0\n",
      "           7       0.00      0.00      0.00      14.0\n",
      "           9       0.00      0.00      0.00      10.0\n",
      "          11       0.00      0.00      0.00       1.0\n",
      "          12       0.00      0.00      0.00       5.0\n",
      "          13       0.00      0.00      0.00       9.0\n",
      "          16       0.00      0.00      0.00      21.0\n",
      "          17       0.00      0.00      0.00      12.0\n",
      "          18       0.00      0.00      0.00       1.0\n",
      "          21       0.00      0.00      0.00       4.0\n",
      "          23       0.00      0.00      0.00       0.0\n",
      "          31       0.00      0.00      0.00       3.0\n",
      "          32       0.00      0.00      0.00       2.0\n",
      "          35       0.00      0.00      0.00      11.0\n",
      "          37       0.00      0.00      0.00       2.0\n",
      "          38       0.00      0.00      0.00       3.0\n",
      "\n",
      "    accuracy                           0.00     122.0\n",
      "   macro avg       0.00      0.00      0.00     122.0\n",
      "weighted avg       0.00      0.00      0.00     122.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\xiang\\miniconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model = load_model('CNN_model_tif.h5')\n",
    "# Load new data (example: assume `new_data.csv` has the same structure as `train_set.csv`)\n",
    "new_data_file = \"../datasets/test_set.csv\"\n",
    "new_df = pd.read_csv(new_data_file)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),  # or any other optimizer you want to use\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Concatenate relevant text features\n",
    "catFamily = new_df[\"Family\"].fillna(\"*empty*\")\n",
    "catSubFamily = new_df[\"SubFamily\"].fillna(\"*empty*\")\n",
    "catObjectGroup = new_df[\"ObjectGroup\"].fillna(\"*empty*\")\n",
    "catObjectName = new_df[\"ObjectName\"].fillna(\"*empty*\")\n",
    "catDescription = new_df[\"Description\"].fillna(\"*empty*\")\n",
    "catTypeComments = new_df[\"Type Comments\"].fillna(\"*empty*\")\n",
    "catStructuralMaterial = new_df[\"Structural Material\"].fillna(\"*empty*\")\n",
    "catMaterial = new_df[\"Material\"].fillna(\"*empty*\")\n",
    "\n",
    "new_df[\"Features\"] = (\n",
    "    catFamily\n",
    "    + \"|\"\n",
    "    + catSubFamily\n",
    "    + \"|\"\n",
    "    + catObjectGroup\n",
    "    + \"|\"\n",
    "    + catObjectName\n",
    "    + \"|\"\n",
    "    + catDescription\n",
    "    + \"|\"\n",
    "    + catTypeComments\n",
    "    + \"|\"\n",
    "    + catStructuralMaterial\n",
    "    + \"|\"\n",
    "    + catMaterial\n",
    ")\n",
    "\n",
    "test2343 = label_encoder.transform(new_df['Category'])\n",
    "\n",
    "\n",
    "# Vectorize text data using the same CountVectorizer instance\n",
    "X_new_tfidf = vectorizer.transform(new_df[\"Features\"].fillna(EMPTYCONST))\n",
    "# Convert X_tfidf to a dense numpy array\n",
    "X_tfidf_new_dense = X_new_tfidf.toarray()\n",
    "X_test_new_reshaped = np.expand_dims(X_tfidf_new_dense, axis=1)    # Shape (batch_size, timesteps, features)\n",
    "\n",
    "# Predict using the loaded model\n",
    "predictions_new = model_nn.predict(X_test_new_reshaped)\n",
    "predicted_classes = predictions_new.argmax(axis=1)  # Assuming predictions_new is one-hot encoded\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test2343, predicted_classes)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(test2343, predicted_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
